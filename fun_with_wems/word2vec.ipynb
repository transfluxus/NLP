{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fun with Word Embeddings\n",
    "__First we need to import all necessary modules. If you see a number in the box on the left after running the cell you are good to go!\n",
    "We're using gensim's word2vec implementation. [Documentation here!](https://radimrehurek.com/gensim/models/word2vec.html)__\n",
    "\n",
    "What's the purpose: What are they good for!\n",
    "\n",
    "__Natural Language Processing__\n",
    "![wiki is up-todate](https://upload.wikimedia.org/wikipedia/commons/8/8b/Automated_online_assistant.png)\n",
    "\n",
    "- Syntactical Analysis: Part-of-speech tagging, Parsing\n",
    "- Semantics: __Lexical semantics__, Machine translation, Named Entity Recognition, Natural language generation, Natural language understanding, Question answering, Sentiment analysis, Topic segmentation and recognition, Automatic summarization, \n",
    "- Sarcasm detection, Emoji2Vec\n",
    "- Recent trend: Use a word embedding and your system is gonna work better!\n",
    "\n",
    "- So where is the fun? ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Jupyter Notebook\n",
    "Next to Jupyter you also need a few other python modules.\n",
    "Get them with pip:\n",
    "\n",
    "`pip install jupyter gensim matplotlib scikit-learn`\n",
    "\n",
    "Download [this notebook](https://transfluxus.github.io/NLP/fun_with_wems/word2vec.ipynb) and use the terminal to go to its directory. Then start Jupyter with\n",
    "`jupyter notebook`\n",
    "And open this notebook...\n",
    "\n",
    "The next part will import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models.word2vec import KeyedVectors\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now we can download a Word Embedding and load it/ or just load it from this folder.__\n",
    "\n",
    "If you want other Word Embeddings:\n",
    "- [Facebook research released a library for Multilingual Unsupervised or Supervised word Embeddings](https://github.com/facebookresearch/MUSE)\n",
    "- [More models you can download staight with in the following cell](https://github.com/RaRe-Technologies/gensim-data)\n",
    "- [Popular: GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "- [Google's Word2Vec](https://code.google.com/archive/p/word2vec/)\n",
    "- [ConceptNet5: Another guy, who says he has the best word embedding](https://github.com/commonsense/conceptnet-numberbatch)\n",
    "- [HistWords: Word Embeddings for Historical Text](https://nlp.stanford.edu/projects/histwords/)\n",
    "- [Spanish Billion Word Corpus and Embeddings](http://crscardellino.me/SBWCE/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the model file if it is not downloaded yet! \n",
    "model_file = api.load(\"glove-wiki-gigaword-200\", return_path=True)\n",
    "print(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model fil into the memory. This might take 1-2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some documentation on [_gensims_ Word2Vec module](https://radimrehurek.com/gensim/models/word2vec.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similar_by_word(\"barcelona\") \n",
    "# or get more words: \n",
    "# model.similar_by_word(\"barcelona\", topn=100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The famous analogy: _king_ - _man_ + _woman_ â‰ˆ X:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar_cosmul(positive=[\"wine\",\"barley\"], negative=[\"beer\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ruling man is called king. What is the ruling woman called?\n",
    "\n",
    "> king - man  = __queen__ - woman\n",
    "\n",
    "> king - man  = __X__ - woman\n",
    "\n",
    "> king - man  + woman = __X__ \n",
    "\n",
    "> `+` king, woman\n",
    "\n",
    "> `-` man\n",
    "\n",
    "----\n",
    "\n",
    "From barley I make beer. How do I make wine?\n",
    "> beer - marley = wine - X\n",
    "\n",
    "> beer - barley + X = wine\n",
    "\n",
    "> X  = wine - beer + barley\n",
    "\n",
    "Ok, ok. Why beer - marley, why not the other way around. or why not beer + marley.\n",
    "something + something else doesn't describe the relation between 2 vectors (words)\n",
    "\n",
    "![from b to a](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5b/Vector_subtraction.png/320px-Vector_subtraction.png)\n",
    "\n",
    "What you substract from what doesn't matter. No matter how you put it, when you ask for something, the equation will always gonna look the same in the end:\n",
    "\n",
    "So instead of `king - man = queen - woman` we could write `man - king = woman - queen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar_cosmul(positive=[\"large\",\"smaller\"], negative=[\"small\"])\n",
    "# model.most_similar_cosmul(positive=[\"large\",\"small\"], negative=[\"smaller\"])\n",
    "# model.most_similar_cosmul(positive=[\"larger\",\"small\"], negative=[\"smaller\"])\n",
    "# model.most_similar_cosmul(positive=[\"merkel\",\"spain\"], negative=[\"germany\"])\n",
    "# model.most_similar_cosmul(positive=[\"berlin\",\"spain\"], negative=[\"barcelona\"])\n",
    "# model.most_similar_cosmul(positive=[\"microsoft\",\"zuckerberg\"], negative=[\"facebook\"])\n",
    "# model.most_similar_cosmul(positive=[\"spain\",\"alps\"], negative=[\"switzerland\"])\n",
    "# model.most_similar_cosmul(positive=[\"europe\",\"noodles\"], negative=[\"asia\"])\n",
    "# model.most_similar_cosmul(positive=[\"spain\",\"nietzsche\"], negative=[\"germany\"])\n",
    "# model.most_similar_cosmul(positive=[\"spain\",\"funny\"], negative=[\"germany\"])\n",
    "\n",
    "# 1.\n",
    "# model.most_similar_cosmul(positive=[\"beer\",\"apple\"], negative=[\"barley\"])\n",
    "# 2.\n",
    "# model.most_similar_cosmul(positive=[\"wine\",\"barley\"], negative=[\"beer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar_cosmul(positive=[\"spain\",\"funny\"], negative=[\"germany\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How can we check if a word is in the model:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_word = \"london\"\n",
    "# so easy (thanks python!)\n",
    "exists_in_model = my_word in model\n",
    "print(\"is %s in the model? - %s\" % (my_word, \"jepp\" if exists_in_model else 'nope')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Which word doesn't fit?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.doesnt_match(\"barcelona madrid england berlin\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__If you just like to look at numbers: How similar are these 2.__ \n",
    "\n",
    "__1: Very similar__\n",
    "    \n",
    "__0: Not similar at all__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similarity('woman', 'man')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What fits best to Barcelona? city, england or pizza?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar_to_given(\"barcelona\",[\"dog\",\"england\",\"pizza\",\"berlin\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What's closer to Barcelona then messi???__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.words_closer_than(\"barcelona\",\"messi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How many words are more similar to barcelona than 'catalonia'__?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rank(\"barcelona\",\"catalonia\")\n",
    "# this is like:\n",
    "# len(model.words_closer_than(\"barcelona\",\"catalonia\")) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's do something more complex now. Let's get the 100 most similar (`topn` in the code) words to barcelona.\n",
    "Then we smash them from their 'k' dimensions down to 2. Then we plot them__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 100 most similar words\n",
    "bcn_sims = model.most_similar_cosmul(positive=[\"barcelona\"], topn=100)\n",
    "print(\"most similar:\",bcn_sims[0]) # print the most similar\n",
    "# just take the words\n",
    "bcn_sims = [\"barcelona\"] + [sim[0] for sim in bcn_sims]\n",
    "print(\"10 most similar words:\",bcn_sims[:10]) # print first 5 words\n",
    "# get the vectors of all these words\n",
    "bcn_sim_vecs = [model[sim] for sim in bcn_sims]\n",
    "print(\"Every vector has %s values... it looks like this:\\n %s\" % (len(bcn_sim_vecs[0]), bcn_sim_vecs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Definition of a function that draws a bunch of 2d points with labels, and colors and circlesize:\n",
    "We use 4 lists which must all have the same length. `colors` and `size` can be ommited.\n",
    "The 4 lists look like:__\n",
    "\n",
    "`data = [[x0, y0], [x1, y1], ...]`\n",
    "\n",
    "`labels = [\"barcelona\", \"something\", ...]`\n",
    "\n",
    "`colors = [0, 1, ...]` Here we just need integer numbers (for the groups of colors)\n",
    "\n",
    "`size = [100, 200, ...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def plot(data,labels,colors = None, size = 50):\n",
    "\n",
    "    if not colors:\n",
    "        colors = [0] * len(data)\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.subplots_adjust(bottom = 0.1)\n",
    "\n",
    "    plt.scatter(\n",
    "        data[:, 0], data[:, 1], marker='o', c=colors, s = size,\n",
    "        cmap=plt.get_cmap('Spectral'))\n",
    "\n",
    "    for label, x, y in zip(labels, data[:, 0], data[:, 1]):\n",
    "        plt.annotate(\n",
    "            label,\n",
    "            xy=(x, y), xytext=(0, 5),\n",
    "            textcoords='offset points', ha='right', va='bottom')\n",
    "    #         bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__t-SNE or t-Distributed Stochastic Neighbor Embedding allows us to reduce the dimensionality of a dataset. Obviously some details get losts... But it's trying to keep points close that were close to each other before the transformation__\n",
    "\n",
    "This is a nice interactive page about the tricky parts of it: https://distill.pub/2016/misread-tsne/\n",
    "\n",
    "This is documentation to the implementation we are using. It explains the parameters you can set:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n",
    "\n",
    "the dimension of the embedded space is per default 2. that's why we don't need to specify the `n_components` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_to_2d = TSNE().fit_transform(bcn_sim_vecs)\n",
    "# documentation: http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n",
    "plot(transform_to_2d,bcn_sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to test a bunch of words and see to which city they are most similar to.\n",
    "\n",
    "__Is pizza more similar to paris, barcelona, brussels or berlin?__\n",
    "\n",
    "We keep these words in `compare_word_list` and our cities in `compare_to`.\n",
    "\n",
    "Then we can use the `most_similar_to_given` function for each word.\n",
    "\n",
    "Then we color code it and draw it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_word_list = [\"pizza\",\"beer\",\"football\",\"peace\",\"war\",\"love\",\n",
    "                     \"santos\",\"beer\",\"dÃ¶ner\",\"sun\",\"fish\",\"winter\",\"king\",\"germany\",\"tower\",\"water\",\"warm\", \"cold\"]\n",
    "\n",
    "compare_to = [\"paris\",\"barcelona\",\"brussels\",\"berlin\", \"rome\"]\n",
    "\n",
    "\n",
    "# check for every word if it exits in the model. otherwise : kick it out\n",
    "for word_list in [compare_word_list, compare_to]:\n",
    "    for ind,word in enumerate(word_list):\n",
    "        if not word in model:\n",
    "            del word_list[ind]\n",
    "            print(\"gotta kick out:\",word)\n",
    "\n",
    "# for each word save the most similar city\n",
    "most_similar = []\n",
    "for word in compare_word_list:\n",
    "    most_similar_to = model.most_similar_to_given(word,compare_to)\n",
    "    most_similar.append(most_similar_to)\n",
    "    print(word,\":\", most_similar_to)\n",
    "    \n",
    "# mix the 2 lists together: words + cities\n",
    "show_words = compare_word_list + compare_to\n",
    "# get the vectors of all words in 2d\n",
    "word_vecs_2d = TSNE().fit_transform([model[word] for word in show_words])\n",
    "# set the color_index for each most_similar and the cities by the index of the city in the list of cities\n",
    "colors = [compare_to.index(sim) for sim in most_similar] + [compare_to.index(word) for word in compare_to]\n",
    "# set the size the circles of all words to 100 and all cities to 300\n",
    "sizes = ([100] * len(compare_word_list)) + ([300] * len(compare_to))\n",
    "# plot it!\n",
    "plot(word_vecs_2d, show_words, colors, sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__In a rudimantary way, we can also get the similarity of sentences.\n",
    "We need to split a sentence into the single words and kick all so-called stopwords out: a, the, by, had, and, so...\n",
    "There is [a whole list in this file](stopwords.txt)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = open('stopwords.txt').read().split('\\n')\n",
    "\n",
    "def sent_sim(sen1, sen2):\n",
    "    words1 = sen1.lower().split()\n",
    "    words1 = [w for w in words1 if w not in stopwords]\n",
    "    words2 = sen2.lower().split()\n",
    "    words2 = [w for w in words2 if w not in stopwords]\n",
    "    return model.n_similarity(words1,words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"Barcelona is a sunny place\"\n",
    "sentence2 = \"Berlin in winter is a great place , not\"\n",
    "sentence3 = \"my cat garfield was chasing a mouse\"\n",
    "\n",
    "print(sent_sim(sentence1, sentence2))\n",
    "print(sent_sim(sentence1, sentence3))\n",
    "print(sent_sim(sentence2, sentence3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soooo. wanna make your own model or what?\n",
    "There is a rule for building models.\n",
    "___Garbage in garbage out___\n",
    "\n",
    "That means if you want your model to be accurate you need to clean your data (text)!\n",
    "\n",
    "What is clean text-data? It depends on your use-case.\n",
    "\n",
    "However, generally you want to remove punctuation, phone numbers, email addresses, stopwords, and maybe even normalize your words. \n",
    "What's normalization? Making them lowercase and lemmatizing.\n",
    "Some lemma examples:\n",
    "- ran âž¤ run (present tense)\n",
    "- runs âž¤ run\n",
    "- has âž¤ have\n",
    "- are âž¤ be\n",
    "- kitchnes âž¤ kitchen (singular)\n",
    "- ... _pretty easy for english_\n",
    "\n",
    "How am I supposed to do that. __There was big NLP library called [NLTK](http://www.nltk.org/) but the new cool kid on the block is [SpaCy](https://spacy.io/)__. Especially [textacy](https://textacy.readthedocs.io) gets the job done! NLTK has a nice collection of corpora tho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Minimum:__ Specify the corpus filepath (a textfile that contains the text) and model filepath (where your model will be saved). There are some more parameters you can tune:\n",
    "```\n",
    "vector_size = 100 # memory (smaller size) vs precision (larger size)  \n",
    "min_count = 5 # throw out all words that appear less then `min_count` times\n",
    "learning_iterations = 20 # iterate over the corpus so many times\n",
    "window = 5 # The context size for each word\n",
    "workers = multiprocessing.cpu_count() # multiprocessing!!!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec, LineSentence\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "# where is your clean text file:\n",
    "corpus_path = \"corpus/corpus_clean.txt\"\n",
    "model_path = 'models/my_model.bin'\n",
    "\n",
    "vector_size = 100 # memory (smaller size) vs precision (larger size)  \n",
    "min_count = 5 # throw out all words that appear less then `min_count` times\n",
    "learning_iterations = 20 # iterate over the corpus so many times\n",
    "window = 5 # The context size for each word\n",
    "workers = multiprocessing.cpu_count() # multiprocessing!!!\n",
    "\n",
    "def build_word2vec(corpus_path, model_path, delete_temporary_files=True, binary=True):\n",
    "    model = Word2Vec(size=vector_size ,min_count=min_count, iter=learning_iterations, window=window, workers=workers)\n",
    "    if os.path.exists(corpus_path):\n",
    "        sentences = LineSentence(corpus_path)\n",
    "    else:\n",
    "        print('no file. bye')\n",
    "        return\n",
    "    print(\"building vocabulary...\")\n",
    "    model.build_vocab(sentences)\n",
    "    print(\"training model...\")\n",
    "    model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)\n",
    "    # call this if you do not need to train the model again\n",
    "    if delete_temporary_files:\n",
    "        model.delete_temporary_training_data(True)\n",
    "    print(\"saving...\")\n",
    "    model.wv.save_word2vec_format(model_path, binary=binary)    \n",
    "    print(\"done\",model)\n",
    "    return model.wv\n",
    "    \n",
    "my_model = build_word2vec(corpus_path,model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = 'models/my_model.bin'\n",
    "# my_model = KeyedVectors.load_word2vec_format(model_path)\n",
    "\n",
    "# all_words = list(my_model.vocab.keys())\n",
    "# print(all_words[1000:1100])\n",
    "print(\"Barcelon? %s\" % ('barcelona' in my_model))\n",
    "my_model.most_similar_cosmul(positive=[\"barcelona\"], negative=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More fun:\n",
    "\n",
    "[How to implement Sentiment Analysis using word embedding and Convolutional Neural Networks on Keras](https://medium.com/@thoszymkowiak/how-to-implement-sentiment-analysis-using-word-embedding-and-convolutional-neural-networks-on-keras-163197aef623)\n",
    "\n",
    "[Trends and future directions](http://ruder.io/word-embeddings-2017/)\n",
    "\n",
    "[Doc2Vec with gensim: Proper sentence similarities](https://rare-technologies.com/doc2vec-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Appendix__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "For the next part you need `textacy`: so grab it with pip: `pip install textacy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cleaning the text\n",
    "from textacy.preprocess import preprocess_text\n",
    "from textacy import fileio \n",
    "\n",
    "text = fileio.read.read_file('corpus/corpus.txt',encoding='utf-8')\n",
    "\n",
    "# super_clean_text = preprocess_text(text, fix_unicode=True, \n",
    "#                 lowercase=True, transliterate=True, \n",
    "#                 no_urls=True, no_emails=True, \n",
    "#                 no_phone_numbers=True, no_numbers=False, \n",
    "#                 no_currency_symbols=True, no_punct=True, \n",
    "#                 no_contractions=True, no_accents=True)\n",
    "\n",
    "# fileio.write.write_file(super_clean_text,'corpus/corpus_super_clean.txt',encoding='utf-8')\n",
    "\n",
    "clean_text = preprocess_text(text, fix_unicode=True, \n",
    "                lowercase=True, transliterate=False, \n",
    "                no_urls=True, no_emails=True, \n",
    "                no_phone_numbers=True, no_numbers=False, \n",
    "                no_currency_symbols=False, no_punct=True, \n",
    "                no_contractions=True, no_accents=False)\n",
    "\n",
    "fileio.write.write_file(clean_text,'corpus/corpus_clean.txt',encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
